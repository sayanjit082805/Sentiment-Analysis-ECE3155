{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1808590,"sourceType":"datasetVersion","datasetId":989445}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers datasets evaluate ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom transformers import TFDistilBertForSequenceClassification","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/sentiment-analysis-dataset/train.csv', encoding='latin-1')\ntest_df = pd.read_csv('/kaggle/input/sentiment-analysis-dataset/test.csv', encoding='latin-1')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove unnecessary columns\ntrain_df.drop(columns=['textID', 'selected_text', 'Time of Tweet', 'Age of User', 'Country', 'Population -2020', 'Land Area (Km²)', 'Density (P/Km²)'], inplace=True)\ntrain_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove unnecessary columns\ntest_df.drop(columns=['textID', 'Time of Tweet', 'Age of User', 'Country', 'Population -2020', 'Land Area (Km²)', 'Density (P/Km²)'], inplace=True)\ntest_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Null values\ntrain_df.dropna(subset=['text'], inplace=True)\nprint(train_df.isna().sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.dropna(subset=['text'], inplace=True)\nprint(test_df.isna().sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clean_text(text: str) -> str:\n    if not isinstance(text, str) or text.strip() == \"\":\n        return \"\"\n    \n    # 1. Remove excessive whitespace (but keep single spaces)\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # 2. Normalize some social media patterns\n    text = re.sub(r'@\\w+', '@USER', text)\n    \n    # 3. Replace URLs with a standard token (optional)  \n    text = re.sub(r'https?://\\S+|www\\.\\S+', 'URL', text)\n    \n    # 3. Basic cleanup - remove non-printable characters\n    text = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', text)\n    \n    # 4. Strip leading/trailing whitespace\n    text = text.strip()\n    \n    return text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['cleaned_text'] = train_df['text'].astype(str).apply(clean_text)\ntest_df['cleaned_text'] = test_df['text'].astype(str).apply(clean_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encoding the labels in the train dataset\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ntrain_df['idx'] = label_encoder.fit_transform(train_df['sentiment'])\ntrain_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encoding the labels in the test dataset\ntest_df['idx'] = label_encoder.transform(test_df['sentiment'])\ntest_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_df, val_df = train_test_split(train_df, test_size=0.1, random_state=1, stratify=train_df['idx'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DistilBERT","metadata":{}},{"cell_type":"code","source":"from transformers import DistilBertTokenizerFast\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\nmax_length = 128\n\ndef tokenize_texts(texts):\n    return tokenizer(\n        list(texts),\n        max_length=max_length,\n        truncation=True,\n        padding='max_length',\n        return_tensors='tf'\n    )\n\ntrain_encodings = tokenize_texts(train_df['cleaned_text'])\nval_encodings = tokenize_texts(val_df['cleaned_text'])\ntest_encodings = tokenize_texts(test_df['cleaned_text'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(train_encodings),\n    train_df['idx'].values\n)).shuffle(1000).batch(32)\nval_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(val_encodings),\n    val_df['idx'].values\n)).batch(32)\ntest_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(test_encodings),\n    test_df['idx'].values\n)).batch(32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_bert = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_bert.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=['accuracy'],\n)\nmodel_bert.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Early stopping\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\",\n    min_delta=0.0001,\n    patience=3,\n    verbose=1,\n    mode=\"auto\",\n    baseline=None,\n    restore_best_weights=True,\n    start_from_epoch=0,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_bert_history = model_bert.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=5,\n    callbacks=[early_stopping]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"validation_accuracy = model_bert.evaluate(val_dataset)\ntest_accuracy = model_bert.evaluate(test_dataset)\n\nprint(f\"validation accuracy : {validation_accuracy[1]:0.4f}\")\nprint(f\"test accuracy : {test_accuracy[1]:0.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Predictions\npredictions_out = model_bert.predict(test_dataset)\nif isinstance(predictions_out, dict):\n    logits = predictions_out[\"logits\"]\nelif hasattr(predictions_out, \"logits\"):\n    logits = predictions_out.logits\nelse:\n    logits = predictions_out\ny_pred = np.argmax(logits, axis=1)\n\n# True labels\ny_true = np.concatenate([y.numpy() for _, y in test_dataset], axis=0)\n\n# Reports\nclass_names = label_encoder.classes_\nprint(\"\\n--- Classification Report ---\")\nprint(classification_report(y_true, y_pred, target_names=class_names))\n\ncm = confusion_matrix(y_true, y_pred)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=class_names, yticklabels=class_names, cbar=False)\nplt.title('DistilBERT')\nplt.ylabel('Actual Labels')\nplt.xlabel('Predicted Labels')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# RoBERTa","metadata":{}},{"cell_type":"code","source":"from transformers import RobertaTokenizerFast, TFRobertaForSequenceClassification","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\nmax_length = 128","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize_texts(texts):\n    return tokenizer(\n        list(texts),\n        max_length=max_length,\n        truncation=True,\n        padding='max_length',\n        return_tensors='tf'\n    )\n\ntrain_encodings = tokenize_texts(train_df['cleaned_text'])\nval_encodings   = tokenize_texts(val_df['cleaned_text'])\ntest_encodings  = tokenize_texts(test_df['cleaned_text'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(train_encodings),\n    tf.cast(train_df['idx'].values, tf.int32)   \n)).shuffle(1000).batch(32)\n\nval_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(val_encodings),\n    tf.cast(val_df['idx'].values, tf.int32)\n)).batch(32)\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(test_encodings),\n    tf.cast(test_df['idx'].values, tf.int32)\n)).batch(32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_roberta = TFRobertaForSequenceClassification.from_pretrained(\n    'roberta-base',\n    num_labels=3   \n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_roberta.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=['accuracy'],\n)\nmodel_roberta.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_roberta_history = model_roberta.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=5,\n    callbacks=[early_stopping]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"validation_accuracy = model_roberta.evaluate(val_dataset)\ntest_accuracy = model_roberta.evaluate(test_dataset)\n\nprint(f\"validation accuracy : {validation_accuracy[1]:0.4f}\")\nprint(f\"test accuracy : {test_accuracy[1]:0.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Predictions\npredictions_out = model_roberta.predict(test_dataset)\nif isinstance(predictions_out, dict):\n    logits = predictions_out[\"logits\"]\nelif hasattr(predictions_out, \"logits\"):\n    logits = predictions_out.logits\nelse:\n    logits = predictions_out\ny_pred = np.argmax(logits, axis=1)\n\n# True labels\ny_true = np.concatenate([y.numpy() for _, y in test_dataset], axis=0)\n\n# Reports\nclass_names = label_encoder.classes_\nprint(\"\\n--- Classification Report ---\")\nprint(classification_report(y_true, y_pred, target_names=class_names))\n\ncm = confusion_matrix(y_true, y_pred)\nplt.figure(figsize=(4, 4))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=class_names, yticklabels=class_names, cbar=False)\nplt.title('RoBERTa')\nplt.ylabel('Actual Labels')\nplt.xlabel('Predicted Labels')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}